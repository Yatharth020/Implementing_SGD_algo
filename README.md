# Implementing--SGD-
Implemented the Stochastic Gradient Descent from scratch for comparing with the SGDRegressor using scikit learn 


## Distribution of error
<img src = https://github.com/yatscool007/Implementing--SGD-/blob/master/Implementation1.PNG>


## Distribution of predicted values
<img src = https://github.com/yatscool007/Implementing--SGD-/blob/master/Imp2.PNG>

## Results and Conclusions
<img src = https://github.com/yatscool007/Implementing--SGD-/blob/master/results.PNG>

- We get the best Mean Squared Error value with manual implementation of SGD keeping the learning rate constant for every epoch.

- The Sklearn implementation fared well than manual sgd where we changed the learning rate with every epoch.This can be attributed to the regularization terms which were used for avoiding overfitting in the data.
 

